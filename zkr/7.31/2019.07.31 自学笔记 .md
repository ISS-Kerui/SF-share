#2019.07.31 自学笔记

##一、EM算法和混合高斯模型

### 1. 什么是隐变量？

在看《统计学方法》的EM算法部分时，书中写道EM算法用于含有隐变量的概率模型参数的极大似然估计，或极大后验概率估计。

什么是隐变量呢？当我们估计算法在做的一些事情，我们要做的其实就是估算出概率模型的参数，概率模型是什么呢？你可以简单把它理解成一个分布，甚至说可以把它理解成一个函数，我们的估计算法就是为了求解出这些函数的参数而存在的。这边借用知乎上的一个例子，希望能够解释清楚隐变量是什么。

如果你站在这个人旁边，你目睹了整个过程：这个人选了哪个袋子、抓出来的球是什么颜色的。然后你把每次选择的袋子和抓出来的球的颜色都记录下来（样本观察值），那个人不停地抓，你不停地记。最终你就可以通过你的记录，推测出每个袋子里每种球颜色的大致比例。并且你记录的越多，推测的就越准（中心极限定理）。然而，抓球的人觉得这样很不爽，于是决定不告诉你他从哪个袋子里抓的球，只告诉你抓出来的球的颜色是什么。这时候，“选袋子”的过程由于你看不见，其实就相当于是一个隐变量。隐变量在很多地方都是能够出现的。现在我们经常说的隐变量主要强调它的“latent”。所以广义上的隐变量主要就是指“不能被直接观察到，但是对系统的状态和能观察到的输出存在影响的一种东西”。所以说，很多人在研究隐变量。以及设计出各种更优(比如如可解释、可计算距离、可定义运算等性质)的隐变量的表示。

#### 2. 为什么要有EM算法

我把EM算法当做最大似然估计的拓展，解决难以给出解析解的最大似然估计（MLE）问题。 考虑**高斯分布**，它的最大似然估计是这样的：

$$ \theta^*=arg\, max_\theta\sum_XlogL(X|\theta)$$

其中，$\theta=(\mu,\delta)​$, $\theta^*=(\mu^*,\delta^*)​$ , $logL(\theta|X)=logP(X;\theta)​$是对数似然函数，分号左边是随机变量，右边是模型参数。$P(X;\theta)​$表示X的概率值函数，它是一个以$\theta​$ 为参数的函数。这里对$\theta​$求导很容易解出$\theta^*​$, logL($\theta|x​$) = logP(X;$\theta​$)是对数似然函数，分号左边是随机变量，右边是模型参数。 $P(X|\theta)​$表示X的概率值函数，它是一个以$\theta​$为参数的函数。这里对$\theta​$ 求导很容易解出 $\theta^*​$ 。

如果这是个含有隐量Z的模型比如混合高斯模型，

$P(X; \theta)=\sum^K_{k=1}π_kN(x;\mu_k, ,\delta_k ) = \sum_ZP(Z;π)P(X|Z;\mu_k ,\delta_k)​$

上面假设共有K个高斯模型混合，每个高斯模型的参数为$\theta_k=(\mu_k ,\delta_k)$, 每个高斯模型占总模型的比重为π_k。隐变量$Z\in\{z1,z2,…,z_k\}$ 表示样本$x_i$来自哪一个高斯分布。$P(Z=z_1)=π1$

可以认为，混合高斯分布的观察值是这样产生的：先以概率$π_k$抽取一个高斯分布$z_k$，再以该高斯分布$N(x;\mu_k ,\delta_k)$去生成观测x。这里的$π_k$就是Z的先验分布P(Z;π)，这样最大似然估计就变成了：

$$\theta^*=arg\, max_\theta\sum_XlogL(X|\theta) = arg\, max_\theta\sum_Xlog\sum_ZP(X,Z|\theta)​$$

但这个表达式求偏导比较困难，因为需要求和、取对数、再求和。 EM算法就是可以把log拿到$\sum_Z​$里层，直接对里面的式子求偏导。

### 3. 什么是E步和M步

为了解决上述问题，引入琴生(Jensen)不等式。log是凹函数(凹函数的几何意义为：在函数f(x)的图象上取任意两点，如果函数图象在这两点之间的部分总在连接这两点线段的下方(凸函数为上方)，那么这个函数就是凹函数)，以隐变量Z的任意函数f(Z)为例：

$$logE|f(Z)| = log\sum_ZP(Z)f(Z)\geq\sum_ZP(Z)logf(Z)=E[logf(Z)]​$$

观察这个公式可以发现，左边为先log再求和，和我们之前算出的混合高斯模型的最大似然估计形式一致，而右边就是我们希望得到的格式，将log放入求和符号中。

$$Max = max_\theta\sum_Xlog\sum_ZP(X,Z;\theta)​$$

这个公式的含义是：改变$\theta$值使得这个极大似然估计最大。而这个式子中，还存在X和Z两个变量。X代表着某一个样本数据，而Z代表着X服从的某一个概率分布。 在执行$\sum_Z$时，可以把X看做是定值，此时我们可以把这个联合分布当做Z的随机变量函数。

为了让上式构成琴生不等式左边的形式，需要让P(X,Z; $\theta$)转换成P(Z)乘上一个f(Z)。我们引入一个关于Z的分布Q(Z)，具体是啥分布还不清楚，但它是一个关于$\theta$的函数，这样上式就可以转换成：

$$Max = max_\theta\sum_Xlog\sum_ZQ(Z;\theta) \cdot \frac  {P(X,Z;\theta)} {Q(Z;\theta)}​$$

$$=max_\theta\sum_XlogE_Q[\frac {P(X,Z;\theta)} {P(Z;\theta)}]​$$

$$\geq max_\theta\sum_XE_Q[log\frac {P(X,Z;\theta)} {P(Z;\theta)}]​$$

$$=max_\theta\sum_X\sum_ZQ(Z;\theta)log\frac {P(X,Z;\theta)} {P(Z;\theta)} ​$$

注：期望的定义是：**离散随机变量的一切可能值与其对应的概率P的乘积之和。**

只有当$$\frac {P(X,Z;\theta)} {P(Z;\theta)}=c(c为任意常数) ​$$，上式地三步才能取等号。而且注意到Q是Z的某一分布，有$\sum_ZQ(Z;\theta)=1​$这个性质，因此就有如下推导：

$$Q(Z；\theta)=\frac {P(X,Z;\theta)}{c} = \frac {P(X,Z;\theta)}{c \cdot \sum_ZQ(Z;\theta)} $$

$$=\frac {P(X,Z;\theta)}{ \sum_Zc \cdot Q(Z;\theta)}= \frac {P(X,Z;\theta)} {\sum_ZP(X,Z;\theta)}​$$

$$= \frac {P(X,Z;\theta)} {P(X;\theta)}=P(Z|X;\theta)​$$

这样我们就把未知的函数Q转换成了$P(Z|X;\theta)$。 这一切的前提是c为常数，而c为常数的前提是先给定X，Z的后验分布。接下来就是要最大化极大似然估计值。

$$\theta^*=argmax_\theta\sum_X\sum_ZP(Z|X;\theta)log \frac {P(X,Z;\theta)} {P(Z|X; \theta)}$$

其中：$$P(X,Z;\theta)=P(Z;π)P(X|Z;\mu,\delta)=π_kN(x_i;\mu_i;\delta_i)​$$

$$P(Z|X;\theta)=\frac {P(X,Z;\theta)} {\sum_ZP(X,Z;\theta)}= \frac {π_kN(x_i;\mu_i;\delta_i)} {\sum^K_{k=1}π_kN(x_i;\mu_i;\delta_i)}$$

到目前为止，我们就可以对$(\mu.\delta,π)$求导，使用迭代方法进行最大化。

**所以，M步的作用就是对极大似然估计进行最大化。**



**-----------------E步的作用—————-----**

$$\theta^{j+1}=argmax_\theta\sum_X\sum_ZQ^{j}logP(X,Z;\theta), 其中Q^{j}=P(Z|X;\theta^{(j)})$$

其实，E步就是求给定X下的条件期望，也就是后验期望最大化。直到$Q^j$为定值(常数)。本质是对琴生不等式右端进行放大，使不等式接近于等式。M步最大化联合分布，通过0梯度，拉格朗日法等方法求极值点，也是一次放大。 一直放大下去就能找到最终所求的参数。

#### 3. 例子讲解EM

现在一个班级里面有50个男生，50个女生。我们假设男生的身高服从正态分布$N(\mu_1,\delta^2_1)​$, 女生的身高服从另一个正态分布：$N(\mu_2,\delta^2_2)​$ 。 但现在男生和女生混在一起，我们只知道每个人的身高，但不知道每个人的性别，现在要**求出两个分布的参数**。

直接利用极大似然估计我们无法求解这个问题，因为我们不知道每个样本属于哪个分布(存在隐变量，性别)。 利用混合高斯模型，我们先假定每个样本属于男生样本和女生样本的概率各为0.5。这时候，我们要求解的公式为：

$P(X; \theta)=\sum^K_{k=1}π_kN(x;\mu_k, ,\delta_k ) = \sum_ZP(Z;π)P(X|Z;\mu_k ,\delta_k)$

我们知道了P(Z;π)均为0.5，求解参数$\mu$和$\delta$。 但是混合高斯模型不方便直接求导求解(先求和再log再求和)，我们希望将log放入到求和符合内求解，这时候就需要运用EM思想。EM算法是先固定参数$\theta$ 求隐变量$\Z$的后验分布(E步)，然后固定隐变量求参数(M步)，交替进行直至收敛。

*提问： 为什么不用梯度下降法直接求解？*

*有了隐变量之后，利用梯度下降法求偏导为0，例如对$\theta$求偏导使其为0，但是$\theta$ 与其他参数纠缠在一起，得不到关于$\theta$ 的显示表达式，所以很难进行。*

根据先验先猜一个$\mu_1$，$\theta_1$, $π_1$和$\mu_2$，$\theta_2$, $π_2$这样就得到了男女的高斯分布，然后就可以判断每个样本在目前条件下是属于哪一个类别(男or女)的。 然后，再将上一步求出的后验分布带入公式，求解出新的$\mu_1$，$\theta_1$, $π_1$和$\mu_2$，$\theta_2$, $π_2$。重复上述步骤，直达收敛。







