# 通用网络模型结构总结

## Inception系列

### Inception v1

网络过深会导致模型参数过多，出现过拟合的问题。inception的思路是增加网络的宽度。使用用多个尺寸的卷积层（1，3，5）去提取特征，这样不仅增加了网络对尺度的适应性，另一方面也减少了网络的参数。它的参数相比于AlexNet少了12倍，相对于VGG-16少了3倍，而它的分类效果不输于VGG16。

### Inception V2

在Inception V1的基础上增加了batch normalization，目的在于加快网络的训练速度，减少过拟合。因为网络训练过程中参数不断改变导致后续每一层输入的分布也发生变化，而学习的过程又要使每一层适应输入的分布，因此我们不得不降低学习率、小心地初始化。作者将分布发生变化称之为internal covariate shift。 加入BN层可以很好的解决这个问题。

第二个改进就是采用两个3\*3的卷积去替代5\*5的卷积，比起V1来说参数量减少了，但层数增加，效果更好。

### Inception V3

主要的改进就是平衡了网络的深度和宽度。宽度和深度适宜的话可以让网络应用到分布式上时具有比较平衡的computational budget。 网络结构最大的变化则是用了1\*n结合n\*1来代替n*n的卷积。

### Inception-resnet

在Inception V3的基础上结合了残差块的思想.

### Xception 

采用了可分离卷积的思路，将卷积拆分为Depthwise conv和 pointwise conv，大大减少了参数量。 具体步骤是：先按不同通道进行一次卷积，生成输入通道数相同数据的feature maps，然后将这些feature maps一起进行二次卷积(1*1)进行通道上的融合。

## ResNet 系列

VGG网络试着探寻了一下深度学习网络的深度究竟可以深几许以能持续地提高分类准确率。我们的一般印象当中，深度学习愈是深（复杂，参数多）愈是有着更强的表达能力。凭着这一基本准则CNN分类网络自Alexnet的7层发展到了VGG的16乃至19层，后来更有了Googlenet的22层。可后来我们发现深度CNN网络达到一定深度后再一味地增加层数并不能带来进一步地分类性能提高，反而会招致网络收敛变得更慢，test dataset的分类准确率也变得更差。这就是网络的退化。 而ResNet模型首次引入了残差块的思想，将模型的深度变得更深，并且相比于vgg有更好的分类效果。

ResNet模型有5个版本，resnet-18，resnet-34，resnet-50，resnet-101，resnet152。当模型深度为50，101，152时，模型使用了bottleneck的设计，就是卷积之前会先用1*1的卷积层降维，最后再用1\*1的卷积层升维，这样做的目的是可以有效的减少计算和参数量。

ResNet使用了一种连接方式叫做“shortcut connection”，顾名思义，shortcut就是“抄近道”的意思。这个就是所谓的”shortcut connection“，也是文中提到identity mapping，将每个残差块的输入与它自己的输出相加。如果网络已经到达最优，继续加深网络，residual mapping将被push为0，只剩下identity mapping，这样理论上网络一直处于最优状态了，网络的性能也就不会随着深度增加而降低了。 同时一部分梯度可以直接回传到上一个残差块，减少了梯度消失的可能，让模型可以充分训练。

## DenseNet

相比ResNet，DenseNet提出了一个更激进的密集连接机制：即互相连接所有的层，具体来说就是每个层都会接受其前面所有层作为其额外的输入。

DenseNet有更少的网络参数，网络更窄 。
这种结构更有利于信息的传递，以及进行反向传播的时候，更有利于减轻梯度消失的情况。

## MobileNet

MobileNet是一种基于流线型结构使用深度可分离卷积来构造轻型权重深度神经网络。

### MobileNet V1

每一层可分离卷积之后都跟一个Relu激活层和BN层。总共含有28层。在全连接层前加入Avg pool层减少模型的参数。

### MobileNet V2

V2在Depth-wise卷积之前新加入了一个PW卷积。这样做是因为DW卷积本身不能够改变通道数，如果上层通道数太小，DW也只能在低维空间提取特征，因此效果不好。现在V2为了改善这个问题，给每个DW之前都配了一个PW，用于升维。

V2去掉了第二个PW的激活函数。因为作者任务激活函数在高维空间能够有效的增加非线性，而在低维空间会破坏特征，不如线性的效果好。





